---
name: Hao Zhang
email: sjtu.haozhang@gmail.com
photo: https://datascience.ucsd.edu/wp-content/uploads/2023/01/hao.png
website: https://people.eecs.berkeley.edu/~hao/
domain: B17
title: Developing Open Datasets, Models, Systems, and Evaluation Tools for Large (Language) Models
bio: "Hao's research interests are in the intersection of machine learning and systems, with the focus on improving the performance and ease-of-use of today‚Äôs distributed ML systems.¬† Hao works actively on democratizing the access of large languge models (LLMs). Hao has created several popular open source LLM projects, such as Alpa, Vicuna, and Fastchat. Hao's previous open-source artifacts in ML systems have been used by organizations such as AI2, Meta, and Google, and parts of Hao's research have been commercialized at multiple start-ups including Petuum and AnyScale."
description: "The rapid advancement of large multimodal models has revolutionized AI systems, resulting in unprecedented levels of intelligence as seen in OpenAI‚Äôs GPT-4. However, despite its performance, the training and architecture details of GPT-4 remain unclear, hindering research and open-source innovation in this field. 
<br>
I propose to build an open platform for large multimodal chat assistants, based on our ongoing open-source effort, Vicuna, an impactful language model that has been widely adopted for building generative AI applications. I propose three thrusts to extend Vicuna as an open and unified platform for large multimodal models:
<ol>
<li>On the system side, I propose an infrastructure for scalable training and high-throughput serving with advanced memory management and parallelization techniques.</li>
<li>On the model side, I aim to build a large multimodal model close to ChatGPT quality, which can also interact with the real world by taking actions and using tools.</li>
<li>On the data and benchmark side, I plan to develop a gamified data collection and benchmark platform with novel data augmentation, data filtering, and ranking methods.</li>
</ol>
If successful, the proposed platform will provide ML developers one-stop-for-all experience for training, serving, and evaluating large multimodal models."
summer: "Work through <a href='https://icml.cc/virtual/2022/tutorial/18440'>https://icml.cc/virtual/2022/tutorial/18440</a> and <a href='https://github.com/RUCAIBox/LLMSurvey'>https://github.com/RUCAIBox/LLMSurvey</a>"
oldstudent: nan
prerequisites: Machine learning
time: Thursday 1-2PM, In-Person üìç HDSI 336
style: Will involve my postdoc or PhD who are interested in talking. Hands-on in the early bootstrap stage and will be hands-off afterwards.
seats: 6
tag: Language Models
---